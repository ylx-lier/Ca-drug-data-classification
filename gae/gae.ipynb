{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.24.17：05\\n#现在这份代码实现了求均值的均方误差，得到全为0，\\n#所以下一步是\\n# 1.改图结构(节点全部连接)\\n# 2.缩小图的规模，同时减小hidden layer的规模，\\n# 看能否得到非0的结果\\n#还需要做的是3.验证encode_copy的每一行里加入行序列，看能不能让decode结果里每一行都不一样\\n先让模型收敛\\n\\n'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"12.24.17：05\n",
    "#现在这份代码实现了求均值的均方误差，得到全为0，\n",
    "#所以下一步是\n",
    "# 1.改图结构(节点全部连接)\n",
    "# 2.缩小图的规模，同时减小hidden layer的规模，\n",
    "# 看能否得到非0的结果\n",
    "#还需要做的是3.验证encode_copy的每一行里加入行序列，看能不能让decode结果里每一行都不一样\n",
    "先让模型收敛\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: torch_geometric in /environment/miniconda3/lib/python3.11/site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in /environment/miniconda3/lib/python3.11/site-packages (from torch_geometric) (3.7.4)\n",
      "Requirement already satisfied: fsspec in /environment/miniconda3/lib/python3.11/site-packages (from torch_geometric) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /environment/miniconda3/lib/python3.11/site-packages (from torch_geometric) (3.1.3)\n",
      "Requirement already satisfied: numpy in /environment/miniconda3/lib/python3.11/site-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /environment/miniconda3/lib/python3.11/site-packages (from torch_geometric) (5.9.8)\n",
      "Requirement already satisfied: pyparsing in /environment/miniconda3/lib/python3.11/site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /environment/miniconda3/lib/python3.11/site-packages (from torch_geometric) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /environment/miniconda3/lib/python3.11/site-packages (from torch_geometric) (4.65.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /environment/miniconda3/lib/python3.11/site-packages (from aiohttp->torch_geometric) (23.2.0)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /environment/miniconda3/lib/python3.11/site-packages (from aiohttp->torch_geometric) (3.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /environment/miniconda3/lib/python3.11/site-packages (from aiohttp->torch_geometric) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /environment/miniconda3/lib/python3.11/site-packages (from aiohttp->torch_geometric) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /environment/miniconda3/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /environment/miniconda3/lib/python3.11/site-packages (from aiohttp->torch_geometric) (4.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /environment/miniconda3/lib/python3.11/site-packages (from jinja2->torch_geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /environment/miniconda3/lib/python3.11/site-packages (from requests->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /environment/miniconda3/lib/python3.11/site-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /environment/miniconda3/lib/python3.11/site-packages (from requests->torch_geometric) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /environment/miniconda3/lib/python3.11/site-packages (from requests->torch_geometric) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_tensor(input_tensor, target_length):\n",
    "    input_tensor = input_tensor.unsqueeze(1) \n",
    "    resized_tensor = F.interpolate(\n",
    "        input_tensor, \n",
    "        size=target_length, \n",
    "        mode='linear', \n",
    "        align_corners=False\n",
    "    )\n",
    "    resized_tensor = resized_tensor.squeeze(1)\n",
    "    \n",
    "    return resized_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "## labels: day45 baseline 0, day 45 others 1, day90 baseline 2, day others 90 3, day 120 baseline 4, day 120 others 5\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "## labels: day45 baseline 0, day 45 others 1, day90 baseline 2, day others 90 3, day 120 baseline 4, day 120 others 5\n",
    "file_folder = \"/home/featurize/work/ylx/MEA/overfitting\"\n",
    "sub_file_list = os.listdir(file_folder)\n",
    "all_data = []\n",
    "for file_name in sub_file_list:  \n",
    "    file_path = os.path.join(file_folder, file_name)\n",
    "    data_sample = {}\n",
    "    cls_mea = file_name.split(\"_\")[-2]\n",
    "    if cls_mea == \"baseline\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    data_sample[\"label\"] = label\n",
    "    df = pd.read_csv(file_path)\n",
    "    data_np = df.values\n",
    "    data_tensor = torch.tensor(data_np, dtype=torch.float32)\n",
    "    target_length = 32 \n",
    "    data_tensor = resize_tensor(data_tensor, target_length)\n",
    "    data_sample[\"data\"] = data_tensor\n",
    "    data_sample[\"label\"] = label\n",
    "    data_sample[\"data_name\"] = file_name\n",
    "    all_data.append(data_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(all_data, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(all_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(x):\n",
    "  min_val = x.min()\n",
    "  max_val = x.max()\n",
    "  normalized_x = (x - min_val) / (max_val - min_val)\n",
    "  return normalized_x\n",
    "\n",
    "# print(\"Min-Max Normalization (0-1):\\n\", x_normalized_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)#GCN需要改数据形式\n",
    "        \n",
    "        #预测输出形式预判问题\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "      \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        num_sample = x.shape[0]\n",
    "        # print(num_sample)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        max_values, _ = torch.max(x, dim=0)  \n",
    "        \n",
    "        \n",
    "        # print(max_values.unsqueeze(0))\n",
    "        return min_max_normalize(max_values).unsqueeze(0), num_sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(256,256)\n",
    "        self.linear2 = nn.Linear(256,256)\n",
    "        self.linear3 = nn.Linear(256,256)\n",
    "        self.linear4 = nn.Linear(256,256)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x, num_samples):\n",
    "        # noise  = 0.1 * torch.randn(num_samples, 256)\n",
    "        # noise  = noise.to(\"cuda\")\n",
    "        # x = x + noise\n",
    "        #x复制n份（repeat）变成一个tensor，\n",
    "        #decode的结果打印出来\n",
    "        #算meansquare，按道理是一样的，任务是验证这一点？\n",
    "        #reproducable，minimal example\n",
    "        x = self.linear1(x) # \n",
    "        x1 = x\n",
    "        x = F.relu(x) \n",
    "        x = self.linear2(x)  # \n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x) + x1# \n",
    "        x = F.relu(x) \n",
    "        x = self.linear4(x)  # \n",
    "        # 激活函数\n",
    "        \n",
    "        return x\n",
    "# class Decoder(torch.nn.Module):\n",
    "#     def __init__(self, num_node_features, hidden_channels):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = GCNConv(num_node_features, hidden_channels)#GCN需要改数据形式\n",
    "        \n",
    "#         #预测输出形式预判问题\n",
    "#         self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "#         self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "#         self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "      \n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         # num_sample = x.shape[0]\n",
    "#         # print(num_sample)\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         # x = F.dropout(x, training=self.training)\n",
    "        \n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         # x = F.dropout(x, training=self.training)\n",
    "\n",
    "#         x = self.conv3(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         # x = F.dropout(x, training=self.training)\n",
    "\n",
    "#         x = self.conv4(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         # x = F.dropout(x, training=self.training)\n",
    "#         # x = global_mean_pool(x, batch)\n",
    "#         # max_values, _ = torch.max(x, dim=0)  \n",
    "        \n",
    "        \n",
    "#         # print(max_values.unsqueeze(0))\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Decoder(torch.nn.Module):\n",
    "#     def __init__(self, num_node_features, hidden_channels):\n",
    "#         super().__init__()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         return x\n",
    "     # 生成标准正态分布的噪声，形状为 (num_samples, 256) \n",
    "# import torch.nn as nn\n",
    "# class Decoder(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.linear1 = nn.Linear(256,4570)\n",
    "#         self.linear2 = nn.Linear(4570,8000)\n",
    "#         self.linear3 = nn.Linear(8000,4570)\n",
    "#         self.linear4 = nn.Linear(4570,4570)\n",
    "        \n",
    "        \n",
    "\n",
    "#     def forward(self, x, num_samples):\n",
    "#         noise  = 0.1 * torch.randn(num_samples, 256)\n",
    "#         noise  = noise.to(\"cuda\")\n",
    "#         x = x+noise\n",
    "#         #x复制n份（repeat）变成一个tensor，\n",
    "#         #decode的结果打印出来\n",
    "#         #算meansquare，按道理是一样的，任务是验证这一点？\n",
    "#         reproducable，minimal example\n",
    "#         x = self.linear1(x) # \n",
    "#         x1 = x\n",
    "#         x = F.relu(x) \n",
    "#         x = self.linear2(x)  # \n",
    "#         x = F.relu(x)\n",
    "#         x = self.linear3(x) + x1# \n",
    "#         x = F.relu(x) \n",
    "#         x = self.linear4(x)  # \n",
    "#         # 激活函数\n",
    "        \n",
    "#         return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(torch.nn.Module):  \n",
    "    def __init__(self, num_node_features, hidden_channels):  \n",
    "        super().__init__()  \n",
    "        self.encoder = Encoder(num_node_features, hidden_channels)  \n",
    "        self.decoder = Decoder() \n",
    "    def forward(self, x, edge_index, batch):  \n",
    "        # Encode the input  \n",
    "        \n",
    "        \n",
    "        encoded, num_sample = self.encoder(x, edge_index, batch)\n",
    "        #added\n",
    "        # print(\"encode_shape:\", encoded.shape)\n",
    "        # print(\"encoded.unsqueeze(0).shape:\", encoded.unsqueeze(0).shape)\n",
    "        # print(\"x.shape: \", x.shape)\n",
    "        # print(\"repeat_shape:\", x.shape[0])\n",
    "        encode_copy = encoded.repeat(x.shape[0], 1)\n",
    "        # print(\"encode_copy:\", encode_copy)  \n",
    "        # print(\"encode_copy.shape\", encode_copy.shape)\n",
    "        decoded = self.decoder(encoded,num_sample)\n",
    "        \n",
    "        \n",
    "        #added\n",
    "        # print(\"decode:\", decoded) \n",
    "        # print(\"decoded.shape\", decoded.shape)\n",
    "        decoded_of_encode_copy = self.decoder(encode_copy, num_sample)\n",
    "        \n",
    "        # print(\"decoded_of_encode_copy:\", decoded_of_encode_copy)\n",
    "        # print(\"decoded_of_encode_copy.shape\", decoded_of_encode_copy.shape)\n",
    "        return decoded_of_encode_copy  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 16\n",
    "num_node_features = 4570\n",
    "hidden_channels = 256\n",
    "model = Autoencoder(num_node_features, hidden_channels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成一个随机图数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import random\n",
    "# from torch_geometric.data import Data\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# import numpy as np\n",
    "\n",
    "# def generate_random_graph(num_nodes_range=(5, 15), feature_dim=5):\n",
    "#     \"\"\"随机生成一个图数据\n",
    "\n",
    "#     Args:\n",
    "#         num_nodes_range:  一个元组(min, max), 表示节点数量的范围\n",
    "#         feature_dim:  节点特征的维度\n",
    "#     Returns:\n",
    "#         Data: 生成的图数据对象\n",
    "#     \"\"\"\n",
    "\n",
    "#     # 1. 随机生成节点数量\n",
    "#     num_nodes = random.randint(num_nodes_range[0], num_nodes_range[1])\n",
    "#     print(\"num_nodes:\", num_nodes)\n",
    "\n",
    "#     # 2. 随机生成节点特征\n",
    "#     node_features = torch.randn(num_nodes, feature_dim)\n",
    "#     print(\"node_features.shape:\", node_features.shape)\n",
    "    \n",
    "#     # 3. 创建链式连接的边索引\n",
    "#     edge_index = []\n",
    "#     for i in range(num_nodes - 1):\n",
    "#         edge_index.append([i, i + 1])\n",
    "#     edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "#     print(\"edge_index.shape:\", edge_index.shape)\n",
    "\n",
    "#     # 4. 随机生成图标签(可选)\n",
    "#     y = torch.tensor([random.randint(0, 1)], dtype=torch.long)\n",
    "\n",
    "#     # 5. 创建 PyTorch Geometric 的 Data 对象\n",
    "#     graph_data = Data(x=node_features, edge_index=edge_index, y=y)\n",
    "#     return graph_data\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # 生成多个随机图\n",
    "#     num_graphs = 10\n",
    "#     graphs = [generate_random_graph() for _ in range(num_graphs)]\n",
    "\n",
    "#     # 随机打乱图的顺序\n",
    "#     random.seed(42)\n",
    "#     random.shuffle(graphs)\n",
    "\n",
    "#     # 划分训练集 (这里我们简单地用全部数据作为训练集)\n",
    "#     train_graphs = graphs\n",
    "#     # test_graphs = graphs[int(num_graphs*0.8):] # 也可以划分测试集\n",
    "\n",
    "#     # 创建数据加载器\n",
    "#     train_loader = DataLoader(train_graphs, batch_size=1, shuffle=False)\n",
    "#     # test_loader = DataLoader(test_graphs, batch_size=32, shuffle=False)\n",
    "\n",
    "#     # 测试加载器\n",
    "#     for batch in train_loader:\n",
    "#         print(\"Batch:\")\n",
    "#         print(\"Node features shape:\", batch.x.shape)\n",
    "#         print(\"Edge indices shape:\", batch.edge_index.shape)\n",
    "#         print(\"Labels:\", batch.y)\n",
    "#         break # 只看一个batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原始数据先放着"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_nodes: 111\n",
      "node_features.shape: torch.Size([111, 4570])\n",
      "torch.Size([2, 110])\n"
     ]
    }
   ],
   "source": [
    "num_graphs = len(all_data) \n",
    "graphs = []\n",
    "for i in range(num_graphs):\n",
    "    graph = all_data[i]\n",
    "    num_nodes = int(graph['data'].shape[0])  # 每个图的节点数目\n",
    "    print(\"num_nodes:\", num_nodes)\n",
    "    node_features = graph['data']            # 节点特征矩阵\n",
    "    print(\"node_features.shape:\", node_features.shape)\n",
    "    # 创建依次连接的边缘索引\n",
    "    edge_index = []\n",
    "    for i in range(num_nodes - 1):\n",
    "        edge_index.append([i, i+1])\n",
    "    # 转换为PyTorch张量\n",
    "    edge_index = torch.tensor(edge_index).t().contiguous()# 转置并连接，[[0,1],[1,2],[2,3],...]变成[[0,1,2,3,...],[1,2,3,4,...]]\n",
    "    # edge_index.append([i+1, i])                      # 如果图是无向的，添加反方向的边\n",
    "    print(edge_index.shape)\n",
    "    y = torch.tensor([graph['label']], dtype=torch.long)\n",
    "    graph_data = Data(x=node_features, edge_index=edge_index, y=y)\n",
    "    graphs.append(graph_data)\n",
    "random.seed(42)\n",
    "random.shuffle(graphs)\n",
    "train_graphs = graphs \n",
    "# test_graphs = graphs[int(num_graphs*0.8):]    # 后20%作为测试集\n",
    "train_loader = DataLoader(train_graphs, batch_size=1, shuffle=False)\n",
    "# test_loader = DataLoader(test_graphs, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1000 [00:00<00:16, 60.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.8462151288986206\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(9.8069e-13, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 63/1000 [00:01<00:15, 60.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, Loss: 850.2485961914062\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(1.8218e-07, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 112/1000 [00:01<00:14, 60.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101, Loss: 60.110633850097656\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(1.6263e-08, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 156/1000 [00:02<00:20, 42.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151, Loss: 110.3248062133789\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(2.0988e-08, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 206/1000 [00:04<00:20, 38.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201, Loss: 9.487380027770996\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(1.1138e-09, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 258/1000 [00:05<00:18, 40.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 251, Loss: 5.209591865539551\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(4.6253e-10, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 307/1000 [00:06<00:19, 36.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301, Loss: 9.795843124389648\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(2.3132e-09, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 355/1000 [00:08<00:17, 37.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 351, Loss: 62.42646408081055\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(1.0569e-08, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 408/1000 [00:09<00:15, 37.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 401, Loss: 80.283935546875\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(1.7521e-08, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 456/1000 [00:10<00:15, 36.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 451, Loss: 757.6495971679688\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(1.6801e-07, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 508/1000 [00:12<00:14, 34.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 501, Loss: 169.71682739257812\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(4.1353e-08, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 556/1000 [00:13<00:13, 33.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 551, Loss: 106.09039306640625\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(2.1795e-08, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 604/1000 [00:15<00:11, 34.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 601, Loss: 243.38800048828125\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(4.8157e-08, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 656/1000 [00:16<00:09, 36.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 651, Loss: 256.93505859375\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(6.0474e-08, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 708/1000 [00:18<00:07, 36.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 701, Loss: 292.755126953125\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(7.1980e-08, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 763/1000 [00:19<00:04, 58.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 751, Loss: 45.14149856567383\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(1.0472e-08, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 812/1000 [00:19<00:03, 61.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 801, Loss: 177.78997802734375\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(3.5643e-08, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 855/1000 [00:20<00:03, 41.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 851, Loss: 230.83462524414062\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(4.7338e-08, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 907/1000 [00:22<00:02, 38.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 901, Loss: 191.63096618652344\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(3.5389e-08, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 955/1000 [00:23<00:01, 34.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 951, Loss: 161.89349365234375\n",
      "mean_squared_difference_of_row_means(decoded_of_encode_copy): tensor(2.9675e-08, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:24<00:00, 40.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# 训练完成后保存模型权重\u001b[39;00m\n\u001b[1;32m     55\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_gcn_model_ep_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_hidden_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_channels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 56\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# 绘制损失曲线\u001b[39;00m\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.11/site-packages/torch/serialization.py:629\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 629\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.11/site-packages/torch/serialization.py:863\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    862\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 863\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import datetime\n",
    "epoch = 1000   \n",
    "learning_rate = 0.01\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, capturable=True)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epoch, eta_min=0.000001)\n",
    "mse_loss = torch.nn.MSELoss() \n",
    "loss_values = []\n",
    "def mean_squared_difference_of_row_means(x):\n",
    "            \"\"\"\n",
    "            计算张量每一行的均方值之间的均方差。\n",
    "\n",
    "            Args:\n",
    "                x: 输入的张量。111x256\n",
    "\n",
    "            Returns:\n",
    "                一个标量，表示每一行均方值之间的均方差。\n",
    "            \"\"\"\n",
    "            column_means = torch.mean(x, dim=0, keepdim=True)\n",
    "            # column_means: 1x256\n",
    "            var = (x-column_means)**2\n",
    "            mse = var.mean(dim=0)\n",
    "            smse = mse.sum()\n",
    "            return smse\n",
    "\n",
    "tttt=torch.randn(1, 256)\n",
    "for epoch in tqdm(range(epoch)):\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        # print(f\"Shape of data.x: {data.x.shape}\")\n",
    "        # encode= encoder(data.x, data.edge_index, data.batch)\n",
    "        # print(f\"Encode shape: {encode.shape}\")\n",
    "        \n",
    "        decode = model(data.x, data.edge_index, data.batch)\n",
    "        \n",
    "        # decode = decoder(encode_copy, data.edge_index, data.batch)\n",
    "        # print(f\"Decoded output shape: {decode.shape}\")\n",
    "\n",
    "        loss = mse_loss(decode, data.x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    loss_values.append(avg_loss)\n",
    "    if epoch%50==0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {avg_loss}')\n",
    "    if epoch%50==0:\n",
    "        print(\"mean_squared_difference_of_row_means(decoded_of_encode_copy):\", mean_squared_difference_of_row_means(decode))\n",
    "\n",
    "    # 训练完成后保存模型权重\n",
    "model_path = f'{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}_gcn_model_ep_{epoch}_lr_{learning_rate}_hidden_{hidden_channels}.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f'Model saved to {model_path}')\n",
    "\n",
    "# 绘制损失曲线\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_values, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "# 保存图像到本地文件\n",
    "loss_curve_path = f'{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}_training_loss_curve_ep_{epoch}_lr_{lerning_rate}_hidden_{hidden_channels}.png'\n",
    "plt.savefig(loss_curve_path)\n",
    "plt.show()\n",
    "\n",
    "# 测试模式\n",
    "# model.eval()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     # for data in test_loader:\n",
    "#     for data in train_loader:\n",
    "\n",
    "#         data = data.to(device)\n",
    "#         out = model(data.x, data.edge_index, data.batch)\n",
    "#         pred = out.argmax(dim=1)\n",
    "#         correct += int((pred == data.y).sum())\n",
    "#         total += data.num_graphs\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     # 遍历训练集或测试集\n",
    "#     for data in train_loader:\n",
    "#         data = data.to(device)\n",
    "        \n",
    "#         # 编码器进行编码\n",
    "#         encode = encoder(data.x, data.edge_index, data.batch)\n",
    "        \n",
    "#         # 假设最终模型需要对编码结果进行分类 (也许解码器不是必要的，取决于你的模型设计)\n",
    "#         # 如果你想对节点进行分类，可以直接用编码结果进行分类\n",
    "#         # 例如，你可能有一个全连接层来将编码结果映射到类别空间：\n",
    "#         # out = model_classifier(encode)\n",
    "        \n",
    "#         # 对于分类任务，我们通常会使用 `argmax` 选择最大概率的类别\n",
    "#         pred = encode.argmax(dim=1)\n",
    "        \n",
    "#         # 计算正确的预测和总预测数\n",
    "#         correct += (pred == data.y).sum().item()\n",
    "#         total += data.num_graphs\n",
    "\n",
    "#     accuracy = correct / total\n",
    "#     print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# accuracy = correct / total\n",
    "# print(f'Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # 加载模型权重进行测试\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "# print(f'Model loaded from {model_path}')\n",
    "\n",
    "# # 收集预测和标签\n",
    "# all_preds = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     # for data in test_loader:     # 确保这里使用的是测试数据加载器\n",
    "#     for data in train_loader:  # 确保这里使用的是测试数据加载器\n",
    "\n",
    "#         # data = data.to(device)\n",
    "#         data = {key: value.to(device) for key, value in data.items()}\n",
    "#         out = model(data.x, data.edge_index, data.batch)\n",
    "#         pred = out.argmax(dim=1)\n",
    "#         all_preds.extend(pred.cpu().tolist())\n",
    "#         all_labels.extend(data.y.cpu().tolist())\n",
    "\n",
    "# # 计算混淆矩阵\n",
    "# conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# # 提取混淆矩阵的值\n",
    "# TN, FP, FN, TP = conf_matrix.ravel()\n",
    "\n",
    "# # 计算准确率 (Accuracy)\n",
    "# accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "# print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# # 计算精确率 (Precision)\n",
    "# precision = TP / (TP + FP)\n",
    "# print(f'Precision: {precision:.4f}')\n",
    "\n",
    "# # 计算召回率 (Recall)\n",
    "# recall = TP / (TP + FN)\n",
    "# print(f'Recall: {recall:.4f}')\n",
    "\n",
    "# # 可视化混淆矩阵\n",
    "# fig, ax = plt.subplots(figsize=(4, 3))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "\n",
    "# # 标注 TP, FP, TN, FN\n",
    "# labels = ['TN', 'FP', 'FN', 'TP']\n",
    "# for i in range(2):\n",
    "#     for j in range(2):\n",
    "#         ax.text(j, i, f\"{labels[i * 2 + j]}={conf_matrix[i, j]}\", ha='center', va='center', color='red')\n",
    "\n",
    "# ax.set_xlabel('Predicted Labels')\n",
    "# ax.set_ylabel('True Labels')\n",
    "# ax.set_title('Confusion Matrix with Annotations')\n",
    "# plt.show()\n",
    "\n",
    "# accuracy = np.sum(np.diag(conf_matrix)) / np.sum(conf_matrix)\n",
    "# print(f'Test Accuracy: {accuracy}')\n",
    "# print(f'all_labels: {all_labels}')\n",
    "# print(f'all_preds: {all_preds}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
