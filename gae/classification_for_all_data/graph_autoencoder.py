# graph_autoencoder.py
import torch
import torch.nn as nn
from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool, global_add_pool
from torch_geometric.data import Data, Batch
import torch.nn.functional as F
from itertools import combinations
import numpy as np

def generate_full_edges(num_nodes):
    """Dynamically generates fully connected edges."""
    edges = list(combinations(range(num_nodes), 2))
    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
    return edge_index
# # generated by gpt, deprecated but maybe useful as a reference afterwards
# class GraphAutoEncoder(nn.Module):
#     def __init__(self, input_dim, hidden_dim, embedding_dim):
#         super(GraphAutoEncoder, self).__init__()
#         self.encoder = Encoder(input_dim, hidden_dim, embedding_dim)
#         self.decoder = Decoder(embedding_dim, hidden_dim, input_dim)

#     def forward(self, x, edge_index):
#         """Forward pass."""
#         z = self.encoder(x, edge_index)  # Encoded node embeddings
#         x_reconstructed = self.decoder(z, edge_index)  # Reconstructed node features
#         graph_embedding = z.mean(dim=0)  # Aggregate embeddings to create graph embedding
#         return z, x_reconstructed, graph_embedding

# class Encoder(nn.Module):
#     def __init__(self, input_dim, hidden_dim, embedding_dim):
#         super(Encoder, self).__init__()
#         self.conv1 = GCNConv(input_dim, hidden_dim)
#         self.conv2 = GCNConv(hidden_dim, embedding_dim)

#     def forward(self, x, edge_index):
#         x = self.conv1(x, edge_index).relu()
#         return self.conv2(x, edge_index)

# class Decoder(nn.Module):
#     def __init__(self, embedding_dim, hidden_dim, output_dim):
#         super(Decoder, self).__init__()
#         self.conv1 = GCNConv(embedding_dim, hidden_dim)
#         self.conv2 = GCNConv(hidden_dim, output_dim)

#     def forward(self, z, edge_index):
#         z = self.conv1(z, edge_index).relu()
#         return self.conv2(z, edge_index)

class Encoder(torch.nn.Module):
    def __init__(self, num_node_features, hidden_channels):
        super().__init__()
        self.conv1 = GCNConv(num_node_features, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.conv3 = GCNConv(hidden_channels, hidden_channels)
        # self.out = torch.nn.Linear(hidden_channels, hidden_channels)
        
    def forward(self, x, edge_index, batch):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = self.conv3(x, edge_index)
        x = F.relu(x)
        # x = self.out(x)
        x = global_mean_pool(x, batch)
        return x

class Decoder(torch.nn.Module):
    def __init__(self, hidden_channels, num_node_features):
        super().__init__()
        self.conv1 = GCNConv(hidden_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.conv3 = GCNConv(hidden_channels, num_node_features)
        # self.out = torch.nn.Linear(num_node_features, num_node_features)
        
    def forward(self, x, edge_index, batch):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = self.conv3(x, edge_index)
        x = F.leaky_relu(x, 0.5)
        # x = self.out(x)
        return x

class GraphAutoEncoder(torch.nn.Module):
    def __init__(self, num_node_features, hidden_channels):
        super().__init__()
        self.encoder = Encoder(num_node_features, hidden_channels)
        self.decoder = Decoder(hidden_channels, num_node_features)
        
    def forward(self, x, edge_index, batch):
        embedding = self.encoder(x, edge_index, batch)
        encoded = embedding.repeat(x.shape[0], 1)
        # def check(feature):
        #     column_mean = feature.mean(dim=0)
        #     mse = ((feature - column_mean)**2).mean(dim=0)
        #     smse = mse.sum()
        #     print(smse.item())
        
            # print("encoded:", encoded)
        decoded = self.decoder(encoded, edge_index, batch)
        # if epoch % 1000 == 0:
        #     check(decoded)
        
            
            
        return embedding, decoded
    
def train_graph_autoencoder(model, graphs, epochs=100, batch_size=8, lr=0.01):
    """Trains the graph autoencoder."""
    """
    Trains the graph autoencoder with batch support.
    Args:
        model: Graph autoencoder model
        graphs: List of graph data
        epochs: Number of training epochs
        batch_size: Batch size for training
        lr: Learning rate
    """
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    model.train()
    data_list = []
    for graph in graphs:
        num_nodes = graph.shape[0]  # Number of nodes
        edge_index = generate_full_edges(num_nodes)  # Fully connected edges
        x = torch.tensor(graph, dtype=torch.float)
        data = Data(x=x, edge_index=edge_index)
        data_list.append(data)
        
    for epoch in range(epochs):
        total_loss = 0
        
        for i in range(0, len(data_list), batch_size):
            # 把生成边的步骤放在循环内部也可以。。先不改了
            batch = Batch.from_data_list(data_list[i:i+batch_size])
            # num_nodes = graph.shape[0]  # Number of nodes
            # edge_index = generate_full_edges(num_nodes)  # Fully connected edges
            # x = torch.tensor(graph, dtype=torch.float)  # Node features

            # Forward pass
            optimizer.zero_grad()
            _, decoded = model(batch.x, batch.edge_index, batch.batch)
            loss = F.mse_loss(decoded, batch.x)  # Reconstruction loss
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}")
    return model

def generate_graph_embeddings(model, graphs, batch_size=8):
    """Generates graph embeddings using the trained model."""
    model.eval()
    embeddings = []
    data_list = []
    for graph in graphs:
        num_nodes = graph.shape[0]
        edge_index = generate_full_edges(num_nodes)
        x = torch.tensor(graph, dtype=torch.float)
        data = Data(x=x, edge_index=edge_index)
        data_list.append(data)
        
    for i in range(0, len(data_list), batch_size):
        # num_nodes = graph.shape[0]
        # edge_index = generate_full_edges(num_nodes)  # Fully connected edges
        # x = torch.tensor(graph, dtype=torch.float)
        batch = Batch.from_data_list(data_list[i:i+batch_size])
        graph_embedding, _ = model(batch.x, batch.edge_index, batch.batch)  # Extract graph embedding
        embeddings.append(graph_embedding.detach().numpy())
    return np.concatenate(embeddings, axis=0)